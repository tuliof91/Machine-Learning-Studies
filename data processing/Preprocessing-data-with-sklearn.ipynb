{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b01cbf-59a6-4f7a-98f2-8611e56a9412",
   "metadata": {},
   "source": [
    "# Data Processing with sklearn\n",
    "\n",
    "*aka feature engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b238404-70ef-4480-b71b-a8c6ebbffc38",
   "metadata": {},
   "source": [
    "Feature engineering the process to represent data that improves the model performance.\n",
    "\n",
    "> It's easy to feed raw data into a machine learning. **The difference is in the data processing**\n",
    "\n",
    "Many factor influence a model performance. It's clear that if a feature doesn't have nay relation with the result, any representation of this data is irrelevant.\n",
    "\n",
    "Keep in mid that different models has it's own needs and limitations. Examples are:\n",
    "\n",
    "- Some models can't process multicolinearity or correlation between features\n",
    "- Many models can't have NANs\n",
    "- Some models are severely penalized with irrelevant features.\n",
    "\n",
    "Feature engineering and feature selection play an important role dealing with this problems.\n",
    "\n",
    "## Data processing workflow\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/875/1*QRVI4dwzTN89P8awT8FC6A.png\"  width=700>\n",
    "\n",
    "## Data pre-processing steps\n",
    "<img src=\"./images/pre-processing_order.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07e0ed-a610-4aa4-af95-fd051d1d6a2e",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "- Split\n",
    "    - Train-test split \n",
    "- Cleaning\n",
    "    - drop irrelevant columns\n",
    "    - remove duplicates\n",
    "    - remove samples based on filter techniques\n",
    "    - remove outliers\n",
    "    - remove incorrect data\n",
    "    - change dtypes\n",
    "    - flag missing as NAN\n",
    "    \n",
    "Data cleaning must be done before missing values imputation and one-hot encoding.\n",
    "\n",
    "- Imput missing values\n",
    "    - SimpleImputer (mean, mode)\n",
    "    - KNNImputer\n",
    "    - IterativeImputer\n",
    "    \n",
    "Imput must be done before one-hot encoding (oht does not accept NANs)\n",
    "\n",
    "- Transform features:\n",
    "    - Categorical Features:\n",
    "        - OrdinalEncoder\n",
    "        - LabelEncoder\n",
    "        - One-Hot Encoding\n",
    "    - Numerical Features:\n",
    "        - Binarizer\n",
    "        - KBinsDiscretizer\n",
    "        - MinMaxScale\n",
    "        - StandardScale\n",
    "        - RobustScale\n",
    "\n",
    "*Nans and dtypes must be done before imputation.\n",
    "\n",
    "- Feature Engineering\n",
    "    - PolynomialFeature\n",
    "    - PowerTransformer\n",
    "    - Feature aggregation (combining features)\n",
    "    \n",
    "- Feature Selection:\n",
    "    - Univariate Statistical Test\n",
    "    - Recursive Feature Engineering (RFE)\n",
    "    - Mutual_info_classifier\n",
    "    - Variance inflation factor (VIF)\n",
    "    \n",
    "- Dimensionality Reduction:\n",
    "    - Principal Component Analysis (PCA)\n",
    "    - Linear Discriminant Analysis (LDA)\n",
    "    - t-SNE\n",
    "    \n",
    "These classes are called **transformers**. The idea is to transform data to ensure it can run the model as well as improve it's performance.\n",
    "\n",
    "> Fit é coisa de crosfiteiro e tem relacao com treino\n",
    "\n",
    "It's important that the transformers are fitted with trainning data to avoid **data leakage** (when test data is used for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b49724-28f7-4cbe-9397-041569589440",
   "metadata": {},
   "source": [
    "Follow this simple recipe:\n",
    "\n",
    "1. Make `train-test split`\n",
    "2. Use `.fit()` on training data\n",
    "3. Use `.transform()` on training data\n",
    "4. Use `.transform()` on test data (with the same transformer trained on the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b87d882-780b-45ac-9173-8f099bd9362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import data viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e0c28-075e-42ad-88cd-0091c4d34930",
   "metadata": {},
   "source": [
    "# Missing data\n",
    "<img src=\"images/pre-processing_order.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a389163-b88e-4081-8446-19ce58ec5b51",
   "metadata": {},
   "source": [
    "## SimpleImputer\n",
    "\n",
    "[**SimpleImputer**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) is an `sklearn.impute` module used to fill missing values. Strategy options are:\n",
    "\n",
    "- `mean`\n",
    "- `median`\n",
    "- `mosrt frequent`\n",
    "- `constant`\n",
    "\n",
    "### Recipe\n",
    "```python\n",
    "# import module\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# instantiate encoder\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# train encoder and transform data\n",
    "X_imp = imputer.fit(X_train)\n",
    "X_imp = imputer.transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a33fa1fa-58c9-4dba-bf1c-8e5102a4c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataset\n",
    "X = np.array([[-1.0], \n",
    "              [-0.5], \n",
    "              [np.nan], \n",
    "              [0.5], \n",
    "              [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91139fa-36eb-422e-b968-3cff8533d748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. ]\n",
      " [-0.5]\n",
      " [ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Instantiate encoder\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Train encoder and transform imput data\n",
    "X_imp = imputer.fit(X)\n",
    "X_imp = imputer.transform(X)\n",
    "\n",
    "# Show Results\n",
    "print(X_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39007c55-6c53-4b94-a025-e86e0ad0876c",
   "metadata": {},
   "source": [
    "## [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer)\n",
    "\n",
    "Missing values are imput with the k nearest neighbors mean in the training data. Two samples are near when non missing features are near.\n",
    "\n",
    "> Do not use for big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba81bed3-7818-45f6-b4bf-a70d9bf3972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2. nan]\n",
      " [ 3.  4.  3.]\n",
      " [nan  6.  5.]\n",
      " [ 8.  8.  7.]]\n"
     ]
    }
   ],
   "source": [
    "# Make data\n",
    "X = np.array([[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea4a0ef-c6c2-420e-8217-45e0360d3bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revcipe\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5590f-257c-414b-97e0-5d415d6acd54",
   "metadata": {},
   "source": [
    "# Transform Features - categorical data \n",
    "<img src=\"images/pre-processing_order.png\" width=600>\n",
    "\n",
    "We already know how to use `pd.get_dummies()`. To make it easyer to integrate this process to a pipeline, it's important to use sklearn.\n",
    "\n",
    "The relevant classes are:\n",
    "- [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)\n",
    "- [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder#sklearn.preprocessing.LabelEncoder)\n",
    "- [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "`LabelEncoder` should only be used on target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ade36-341b-45d2-9c72-889cd24697bb",
   "metadata": {},
   "source": [
    "## OrdinalEncoder and LabelEncoder\n",
    "\n",
    "Transforms categories in an ordered sequence, the order is chosen alphabetically. For an ordered list, `OrdinalEncoder` should be used, and an order array must be passed.\n",
    "\n",
    "<img src=\"https://i.imgur.com/tEogUAr.png\" width=1000>\n",
    "\n",
    "### What's the difference between them?\n",
    "- `OrdinalEncoder` is used for 2D data with shape(n_samples, n_features) and therefore is used in feature transformation;\n",
    "\n",
    "- `LabelEncoder` is for 1D data with shape(n_samples), and therefore it's used for label transformation (Target)\n",
    "\n",
    "Another difference is the learned parameter:\n",
    "- `LabelEncoder` learns `classes_`\n",
    "- `OrdinalEncoder` learns `categories_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e47dfb64-f7fa-41c2-8c67-1a41d8166214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "X = np.array([['Paris', 'Île-de-France', 105.4],\n",
    "              ['Yvelines', 'Île-de-France', 2284.0],\n",
    "              ['Grenoble', 'Auvergne-Rhône-Alpes', 18.13],\n",
    "              ['Lyon Metropolis', 'Auvergne-Rhône-Alpes', 533.68]])\n",
    "\n",
    "# Which ones are cool and uncool?\n",
    "y = ['uncool', 'cool', 'uncool', 'cool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74b7171c-56be-4f24-9134-bf891b876718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Instantiate encoders\n",
    "feature_encoder = OrdinalEncoder()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Train and transform encoders\n",
    "X_encoded = feature_encoder.fit_transform(X[:, :2]) # choose 2 first columns (categorical)\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81e35535-d709-4ea1-ab24-841b57551b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [['Paris' 'Île-de-France' '105.4']\n",
      " ['Yvelines' 'Île-de-France' '2284.0']\n",
      " ['Grenoble' 'Auvergne-Rhône-Alpes' '18.13']\n",
      " ['Lyon Metropolis' 'Auvergne-Rhône-Alpes' '533.68']]\n",
      "\n",
      "\n",
      "X_encoded:\n",
      " [[2. 1.]\n",
      " [3. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]]\n",
      "\n",
      "categories:  [array(['Grenoble', 'Lyon Metropolis', 'Paris', 'Yvelines'], dtype='<U32'), array(['Auvergne-Rhône-Alpes', 'Île-de-France'], dtype='<U32')]\n",
      "\n",
      "\n",
      "y_encoded: \n",
      " [1 0 1 0]\n",
      "\n",
      "classes:  ['cool' 'uncool']\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "print(\"X:\\n\", X)\n",
    "\n",
    "# Result X\n",
    "print(\"\\n\\nX_encoded:\\n\", X_encoded)\n",
    "print(\"\\ncategories: \", feature_encoder.categories_)\n",
    "\n",
    "# Result y\n",
    "print(\"\\n\\ny_encoded: \\n\", y_encoded)\n",
    "print(\"\\nclasses: \", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef69c8e9-cbb8-481e-a94c-1ab90c08a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give OrdinalEncoder an specific order:\n",
    "OrdinalEncoder(categories=[['cold', 'warm', 'hot']])\\\n",
    ".fit_transform([['hot'], ['warm'], ['warm'], ['cold']])\\\n",
    ".reshape((1,-1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5496bf20-17fe-4061-8430-eeee1249f7b4",
   "metadata": {},
   "source": [
    "## [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "Transforms each category in a binary column.\n",
    "\n",
    "<img src=\"https://i.imgur.com/TW5m0aJ.png\" width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9ad79b0-148f-4f58-8167-db8f45a2b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "X = np.array([['abacate'], \n",
    "              ['Irmão do Jorel'], \n",
    "              ['Vovó Juju'], \n",
    "              ['Vovó Juju'], \n",
    "              ['Irmão do Jorel']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a22b7b19-941a-4a74-ab50-662a09338ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 0)\t1.0\n",
      " \n",
      "[array(['Irmão do Jorel', 'Vovó Juju', 'abacate'], dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Instantiate encoder\n",
    "dummy_encoder = OneHotEncoder()\n",
    "\n",
    "# Train encoder\n",
    "X_onehot = dummy_encoder.fit_transform(X)#.toarray()\n",
    "\n",
    "# Show results\n",
    "print(X_onehot)\n",
    "print(' ')\n",
    "print(dummy_encoder.categories_) # order with ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a86c4-d4a4-4048-ac4c-f435415c2a18",
   "metadata": {},
   "source": [
    "OneHotEncoder returns a scipy sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eeae0de5-ce5d-45a0-bc57-737c9dd7d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af62de-ba56-4b98-98ab-c07c3403eb21",
   "metadata": {},
   "source": [
    "A sparce matrix is one way to represent a matrix. Inside the parêntesis the position (line, column) is written and outside the atributed value.\n",
    "\n",
    "**To return a numpy array `sparse=False` is used or apply `X_onehot.toarray()`.**\n",
    "\n",
    "`drop='first'` is used to to exclude the first column. **check documentation before implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b325b6af-8d8b-4b8d-ae42-3f28f0a55330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "X = np.array([['abacate'], \n",
    "              ['Irmão do Jorel'], \n",
    "              ['Vovó Juju'], \n",
    "              ['Vovó Juju'], \n",
    "              ['Irmão do Jorel']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "741b2ded-0944-4a8f-8cf9-68f682c46d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      " \n",
      "[array(['Irmão do Jorel', 'Vovó Juju', 'abacate'], dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate encoder\n",
    "dummy_encoder = OneHotEncoder(drop='first', sparse='False')\n",
    "\n",
    "# Train encoder and transform imput features\n",
    "X_onehot = dummy_encoder.fit_transform(X)\n",
    "\n",
    "# Show results\n",
    "print(X_onehot)\n",
    "print(' ')\n",
    "print(dummy_encoder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81652a7e-1f5b-4047-8b91-6da1cd00f5b6",
   "metadata": {},
   "source": [
    "# Transform Features - Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e51cc2-08b0-47db-b64f-e97a50fe2d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
